# Default configuration for the Taiko Transformer model

model:
  d_model: 256
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 1024
  dropout: 0.1
  audio_feature_size: 80 # Determined by the pre-computed .npy files

data:
  max_sequence_length: 512
  # The tokenizer and audio processing must be aligned.
  # We use 100ms as the quantization step.
  time_quantization_ms: 100
  # The resolution of the source data, used for framing.
  source_resolution_ms: 23.2

training:
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 50 # Increased from 25 for more robust training
  save_path: "output/taiko_transformer.pth"

  # --- New Infrastructure Settings ---
  # Learning rate scheduler settings (ReduceLROnPlateau)
  scheduler:
    patience: 3      # How many epochs to wait for improvement
    factor: 0.5        # Factor by which to reduce the learning rate
    min_lr: 0.000001   # Minimum learning rate

  # Early stopping settings
  early_stopping:
    patience: 7      # How many epochs to wait for improvement before stopping
    min_delta: 0.001   # Minimum change in validation loss to be considered an improvement

  # Cross-validation settings
  k_folds: 5

# --- Development Settings ---
dry_run: false # Set to true to run a quick test with one fold and fewer batches

# --- Reward Model Settings ---
reward_model:
  input_size: 256 # Should match the generator's d_model
  hidden_size: 128

# --- RLHF / PPO Fine-Tuning Settings ---
rlhf:
  ppo_epochs: 10
  ppo_learning_rate: 0.00001
  clip_epsilon: 0.2
  # GAE and Adaptive KL settings
  gamma: 0.99
  lambda_gae: 0.95
  adaptive_kl_coeff: true
  target_kl: 0.1
  # Rollout settings
  max_generation_length: 128
  num_rollouts: 4